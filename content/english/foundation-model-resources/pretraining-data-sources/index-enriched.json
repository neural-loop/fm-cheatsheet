[
  {
    "title": "BIG-bench",
    "url": "https://github.com/google/BIG-bench",
    "description_detail": "The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities.",
    "license": "Apache License 2.0",
    "homepage": "",
    "logo_path": "images/big-bench.png",
    "description-github": "Beyond the Imitation Game collaborative benchmark for measuring and extrapolating the capabilities of language models",
    "stars": 2447
  },
  {
    "title": "D4RL",
    "url": "https://github.com/Farama-Foundation/D4RL",
    "description_detail": "D4RL is an open-source benchmark for offline reinforcement learning.",
    "license": "Apache License 2.0",
    "homepage": "",
    "logo_path": "images/d4rl.png",
    "description-github": "A collection of reference environments for offline reinforcement learning",
    "stars": 1088
  },
  {
    "title": "EvadeML",
    "url": "https://github.com/mzweilin/EvadeML-Zoo",
    "description_detail": "A benchmarking and visualization tool for adversarial ML.",
    "license": "MIT License",
    "homepage": "https://evadeML.org/zoo",
    "logo_path": "images/evademl.jpeg",
    "description-github": "Benchmarking and Visualization Tool for Adversarial Machine Learning",
    "stars": 179
  },
  {
    "title": "EvalAI",
    "url": "https://github.com/Cloud-CV/EvalAI",
    "description_detail": "EvalAI is an open source platform for evaluating and comparing AI algorithms at scale.",
    "license": "Other",
    "homepage": "https://eval.ai",
    "logo_path": "images/evalai.png",
    "description-github": ":cloud: :rocket: :bar_chart: :chart_with_upwards_trend: Evaluating state of the art in AI",
    "stars": 1610
  },
  {
    "title": "Evals",
    "url": "https://github.com/openai/evals",
    "description_detail": "Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.",
    "license": "Other",
    "homepage": "",
    "logo_path": "images/evals.png",
    "description-github": "Evals is a framework for evaluating LLMs and LLM systems, and an open-source registry of benchmarks.",
    "stars": 12832
  },
  {
    "title": "Evaluate",
    "url": "https://github.com/huggingface/evaluate",
    "description_detail": "Evaluate is a library that makes evaluating and comparing models and reporting their performance easier and more standardized.",
    "license": "Apache License 2.0",
    "homepage": "https://huggingface.co/docs/evaluate",
    "logo_path": "images/evaluate.png",
    "description-github": "\ud83e\udd17 Evaluate: A library for easily evaluating machine learning models and datasets.",
    "stars": 1641
  },
  {
    "title": "Helm",
    "url": "https://github.com/stanford-crfm/helm",
    "description_detail": "Holistic Evaluation of Language Models (HELM) is a benchmark framework to increase the transparency of language models.",
    "license": "Apache License 2.0",
    "homepage": "https://crfm.stanford.edu/helm",
    "logo_path": "images/helm.png",
    "description-github": "Holistic Evaluation of Language Models (HELM), a framework to increase the transparency of language models (https://arxiv.org/abs/2211.09110). This framework is also used to evaluate text-to-image models in Holistic Evaluation of Text-to-Image Models (HEIM) (https://arxiv.org/abs/2311.04287).",
    "stars": 1405
  },
  {
    "title": "Lucid",
    "url": "https://github.com/tensorflow/lucid",
    "description_detail": "Lucid is a collection of infrastructure and tools for research in neural network interpretability.",
    "license": "Apache License 2.0",
    "homepage": "",
    "logo_path": "images/lucid.png",
    "description-github": "A collection of infrastructure and tools for research in neural network interpretability.",
    "stars": 4587
  },
  {
    "title": "Meta-World",
    "url": "https://github.com/Farama-Foundation/Metaworld",
    "description_detail": "Meta-World is an open-source simulated benchmark for meta-reinforcement learning and multi-task learning consisting of many distinct robotic manipulation tasks.",
    "license": "MIT License",
    "homepage": "https://meta-world.github.io/",
    "logo_path": "images/meta-world.png",
    "description-github": "Collections of robotics environments geared towards benchmarking multi-task and meta reinforcement learning",
    "stars": 1034
  },
  {
    "title": "OmniSafe",
    "url": "https://github.com/PKU-MARL/omnisafe",
    "description_detail": "OmniSafe is a comprehensive and reliable benchmark for safe reinforcement learning, covering a multitude of SafeRL domains and delivering a new suite of testing environments.",
    "license": "Apache License 2.0",
    "homepage": "https://www.omnisafe.ai",
    "logo_path": "images/omnisafe.png",
    "description-github": "OmniSafe is an infrastructural framework for accelerating SafeRL research.",
    "stars": 763
  },
  {
    "title": "OpenCV Zoo and Benchmark",
    "url": "https://github.com/opencv/opencv_zoo",
    "description_detail": "A zoo for models tuned for OpenCV DNN with benchmarks on different platforms.",
    "license": "Apache License 2.0",
    "homepage": "",
    "logo_path": "images/opencv-zoo-and-benchmark.png",
    "description-github": "Model Zoo For OpenCV DNN and Benchmarks.",
    "stars": 459
  },
  {
    "title": "Overcooked-AI",
    "url": "https://github.com/HumanCompatibleAI/overcooked_ai",
    "description_detail": "Overcooked-AI is a benchmark environment for fully cooperative human-AI task performance, based on the wildly popular video game Overcooked.",
    "license": "MIT License",
    "homepage": "https://arxiv.org/abs/1910.05789",
    "logo_path": "images/overcooked-ai.png",
    "description-github": "A benchmark environment for fully cooperative human-AI performance.",
    "stars": 577
  },
  {
    "title": "Recommenders",
    "url": "https://github.com/Microsoft/Recommenders",
    "description_detail": "Recommenders contains benchmark and best practices for building recommendation systems, provided as Jupyter notebooks.",
    "license": "MIT License",
    "homepage": "https://microsoft-recommenders.readthedocs.io/en/latest/",
    "logo_path": "images/recommenders.png",
    "description-github": "Best Practices on Recommendation Systems",
    "stars": 16877
  },
  {
    "title": "RLeXplore",
    "url": "https://github.com/yuanmingqi/rl-exploration-baselines",
    "description_detail": "RLeXplore provides stable baselines of exploration methods in reinforcement learning",
    "license": "MIT License",
    "homepage": "",
    "logo_path": "images/rlexplore.jpeg",
    "description-github": "RLeXplore provides stable baselines of exploration methods in reinforcement learning, such as intrinsic curiosity module (ICM), random network distillation (RND) and rewarding impact-driven exploration (RIDE).",
    "stars": 295
  },
  {
    "title": "SafePO-Baselines",
    "url": "https://github.com/PKU-MARL/Safe-Policy-Optimization",
    "description_detail": "SafePO-Baselines is a benchmark repository for safe reinforcement learning algorithms.",
    "license": "Apache License 2.0",
    "homepage": "https://safe-policy-optimization.readthedocs.io/en/latest/index.html",
    "logo_path": "images/safepo-baselines.png",
    "description-github": "NeurIPS 2023: Safe Policy Optimization: A benchmark repository for safe reinforcement learning algorithms",
    "stars": 254
  }
]
